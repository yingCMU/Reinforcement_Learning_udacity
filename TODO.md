1. OpenAI Gym's Taxi-v2 environment [link](https://classroom.udacity.com/nanodegrees/nd893/parts/8f607726-757e-4ef5-8b64-f2368755b89a/modules/a85374fa-6a60-425b-a480-85b211c5bd5d/lessons/508d0cf2-7545-48e8-95a0-7ac9467cfad7/concepts/ddb32ccb-2ae0-4c9d-82b0-f45e07271beb)
3. 11.7 discretization [coding](https://classroom.udacity.com/nanodegrees/nd893/parts/8f607726-757e-4ef5-8b64-f2368755b89a/modules/a85374fa-6a60-425b-a480-85b211c5bd5d/lessons/d09af343-a93a-4146-b6ed-b4d5fe762480/concepts/fe263343-c140-4670-bbe7-ed15332422a6)
4. 11.9 [Tile Coding](https://classroom.udacity.com/nanodegrees/nd893/parts/8f607726-757e-4ef5-8b64-f2368755b89a/modules/a85374fa-6a60-425b-a480-85b211c5bd5d/lessons/d09af343-a93a-4146-b6ed-b4d5fe762480/concepts/69805287-b3fd-40a5-8f80-701a38eb0e49)
5. Pytorch:
- [pytorch tutorial](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)
- extra curriculam
6. read more about  Greedy in the Limit with Infinite Exploration (GLIE) conditions: Sarsa(0) (or Sarsa) is an on-policy TD control method. It is guaranteed to converge to the optimal action-value function , as long as the step-size parameter \alphaα is sufficiently small and \epsilonϵ is chosen to satisfy the Greedy in the Limit with Infinite Exploration (GLIE) conditions.
7. on-policy vs off-policy:
 On-policy TD control methods (like Expected Sarsa and Sarsa) have better online performance than off-policy TD control methods (like Q-learning).
Expected Sarsa generally achieves better performance than Sarsa.
1. wrtie MC prediction & control by hand again , find in github
2. TD in clickwalkEnv: cliff-walking task in Example 6.6 of the textbook.
    - When you have finished, you can learn more about the environment in its corresponding [GitHub file](https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py)
